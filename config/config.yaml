# Model dimensions - Small model for quick training
n_embd: 256
num_heads: 8
n_layers: 6
vocab_size: 64  # Will be updated dynamically based on actual dataset
block_size: 256
ffn_hidden_mult: 2
dropout: 0.1

# Module selections
attention_type: flash_attention
attention_class: FlashAttention

ffn_type: ff_swiglu
ffn_class: GatedFeedForward

norm_type: rmsnorm
norm_class: RMSNorm

pos_type: sinusoidal
pos_max_len: 512

# Training params - Optimized for quick training
batch_size: 128
learning_rate: 1e-3
weight_decay: 0.01
warmup_steps: 100
max_steps: 2000

# Misc
seed: 42

# Dataset
dataset_path: "data/input.txt"
