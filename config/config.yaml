# Model dimensions - Scaled up to ~20M parameters
n_embd: 512
num_heads: 8
n_layers: 12
vocab_size: 64  # Will be updated dynamically based on actual dataset
block_size: 512
ffn_hidden_mult: 4
dropout: 0.1

# Module selections
attention_type: flash_attention
attention_class: FlashAttention

ffn_type: ff_swiglu
ffn_class: GatedFeedForward

norm_type: rmsnorm
norm_class: RMSNorm

pos_type: sinusoidal
pos_max_len: 512

# Training params - Optimized for 20M parameter model
batch_size: 32
learning_rate: 3e-4
weight_decay: 0.01
warmup_steps: 500
max_steps: 10000

# Misc
seed: 42

# Dataset
dataset_path: "data/input.txt"
