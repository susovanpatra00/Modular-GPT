# Model dimensions
n_embd: 768
num_heads: 12
n_layers: 12
vocab_size: 50257
block_size: 1024
ffn_hidden_mult: 4
dropout: 0.1

# Module selections
attention_type: flash_attention
attention_class: FlashAttention

ffn_type: ff_swiglu
ffn_class: GatedFeedForward

norm_type: rmsnorm
norm_class: RMSNorm

pos_type: sinusoidal
pos_max_len: 2048

# Training params
batch_size: 8
learning_rate: 3e-4
weight_decay: 0.01
warmup_steps: 1000
max_steps: 100000

# Misc
seed: 42

# Dataset
dataset_path: "data/input.txt"
